apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: opentelemetry
  namespace: observability
spec:
  securityContext:
    privileged: true
    runAsUser: 0
  mode: daemonset
  targetAllocator:
    enabled: true
    allocationStrategy: per-node
    #filterStrategy: relabel-config
    #observability:
    #  metrics: {}
    prometheusCR:
      enabled: true
      serviceMonitorSelector: {}
      podMonitorSelector: {}
  envFrom:
    - configMapRef:
        name: stage
  env:
    - name: "OTEL_COLLECTOR_NAME"
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    - name: "K8S_NODE_NAME"
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  resources:
    requests:
      memory: 1000Mi
      cpu: 100m
    limits:
      memory: 2000Mi
      cpu: 300m

  volumes:
    - name: varlogpods
      hostPath:
        path: /var/log/pods
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
    - name: hostfs
      hostPath:
        path: /
    - name: geoip
      persistentVolumeClaim:
        claimName: geoip

  volumeMounts:
    - name: varlogpods
      mountPath: /var/log/pods
      readOnly: true
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: hostfs
      mountPath: /hostfs
      readOnly: true
      mountPropagation: HostToContainer
    - name: geoip
      mountPath: /geoip

#  initContainers:
#    - name: geoipupdate
#      image: maxmindinc/geoipupdate:v7.1.0
#      imagePullPolicy: IfNotPresent
#      envFrom:
#        - secretRef:
#            name: geoipupdate
#      env:
#        - name: GEOIPUPDATE_VERBOSE
#          value: "1"
#        - name: GEOIPUPDATE_EDITION_IDS
#          value: "GeoLite2-ASN GeoLite2-City GeoLite2-Country"
#      volumeMounts:
#        - name: geoip
#          mountPath: /usr/share/GeoIP

  config:

    receivers:

      filelog:
        exclude:
          - /var/log/pods/observability_opentelemetry-collector*_*/opentelemetry-collector/*.log
        include:
          - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        retry_on_failure:
          enabled: true
        start_at: end
        operators:
          # Find out which format is used by kubernetes
          - type: router
            id: get-format
            routes:
              - output: parser-docker
                expr: 'body matches "^\\{"'
              - output: parser-crio
                expr: 'body matches "^[^ Z]+ "'
              - output: parser-containerd
                expr: 'body matches "^[^ Z]+Z"'
          # Parse CRI-O format
          - type: regex_parser
            id: parser-crio
            regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
            timestamp:
              parse_from: attributes.time
              layout_type: gotime
              layout: '2006-01-02T15:04:05.999999999Z07:00'
          - type: recombine
            id: crio-recombine
            output: extract_metadata_from_filepath
            combine_field: attributes.log
            source_identifier: attributes["log.file.path"]
            is_last_entry: "attributes.logtag == 'F'"
            combine_with: ""
            max_log_size: 102400
          # Parse CRI-Containerd format
          - type: regex_parser
            id: parser-containerd
            regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          - type: recombine
            id: containerd-recombine
            output: extract_metadata_from_filepath
            combine_field: attributes.log
            source_identifier: attributes["log.file.path"]
            is_last_entry: "attributes.logtag == 'F'"
            combine_with: ""
            max_log_size: 102400
          # Parse Docker format
          - type: json_parser
            id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
            parse_from: attributes["log.file.path"]
          # Rename attributes
          - type: move
            from: attributes.stream
            to: attributes["log.iostream"]
          - type: move
            from: attributes.container_name
            to: resource["k8s.container.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.pod_name
            to: resource["k8s.pod.name"]
          - type: move
            from: attributes.restart_count
            to: resource["k8s.container.restart_count"]
          - type: move
            from: attributes.uid
            to: resource["k8s.pod.uid"]
          # Clean up log body
          - type: move
            from: attributes.log
            to: body

      hostmetrics:
        collection_interval: 10s
        root_path: /hostfs
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
          disk: {}
          filesystem:
            exclude_fs_types:
              fs_types:
                - autofs
                - binfmt_misc
                - bpf
                - cgroup2
                - configfs
                - debugfs
                - devpts
                - devtmpfs
                - fusectl
                - hugetlbfs
                - iso9660
                - mqueue
                - nsfs
                - overlay
                - proc
                - procfs
                - pstore
                - rpc_pipefs
                - securityfs
                - selinuxfs
                - squashfs
                - sysfs
                - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
                - /dev/*
                - /proc/*
                - /sys/*
                - /run/k3s/containerd/*
                - /var/lib/docker/*
                - /var/lib/kubelet/*
                - /snap/*
            metrics:
              system.filesystem.utilization:
                enabled: true
          load: {}
          memory: {}
          network: {}
          paging:
            metrics:
              system.paging.utilization:
                enabled: true
          processes: {}

      prometheus:
        config:
          scrape_configs:
#            - job_name: 'apiserver'
#              scrape_interval: 60s
#              kubernetes_sd_configs:
#                - role: node # normally endpoints, but temporary due to a limitation in the targetallocator
#              relabel_configs:
#                - replacement: kubernetes.default.svc.cluster.local:443
#                  target_label: __address__
#                - regex: (.+)
#                  replacement: /api/v1/nodes/$${1}/proxy/metrics
#                  source_labels:
#                    - __meta_kubernetes_node_name
#                  target_label: __metrics_path__
#                - source_labels: [__meta_kubernetes_node_name]
#                  target_label: node  # Add node label with current node name
#              scheme: https
#              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
#              tls_config:
#                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#                insecure_skip_verify: false
#                server_name: kubernetes
            - job_name: 'cadvisor'
              scrape_interval: 60s
              kubernetes_sd_configs:
                - role: node
              relabel_configs:
                - replacement: kubernetes.default.svc.cluster.local:443
                  target_label: __address__
                - regex: (.+)
                  replacement: /api/v1/nodes/$${1}/proxy/metrics/cadvisor
                  source_labels:
                    - __meta_kubernetes_node_name
                  target_label: __metrics_path__
                - source_labels: [__meta_kubernetes_node_name]
                  target_label: node  # Add node label with current node name
              scheme: https
              honor_labels: true
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
                server_name: kubernetes

      kubeletstats:
        collection_interval: 30s
        auth_type: 'serviceAccount'
        endpoint: '${env:K8S_NODE_NAME}:10250'
        node: '${env:K8S_NODE_NAME}'
        insecure_skip_verify: true
        extra_metadata_labels:
          - container.id
          - k8s.volume.type
        metric_groups:
          - node
          - pod
          - container
          - volume
        k8s_api_config:
          auth_type: serviceAccount
        metrics:
          k8s.container.cpu.node.utilization:
            enabled: true
          k8s.pod.cpu.node.utilization:
            enabled: true
          k8s.container.memory.node.utilization:
            enabled: true
          k8s.pod.memory.node.utilization:
            enabled: true

      k8s_cluster:
        auth_type: serviceAccount
        node_conditions_to_report:
          - Ready
          - MemoryPressure
        allocatable_types_to_report:
          - cpu
          - memory
          - storage
          - ephemeral-storage

      k8s_events: {}

      k8sobjects:
        objects:
          - name: pods
            mode: pull
          - name: events
            mode: watch
          - group: events.k8s.io
            mode: watch
            name: events

      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      otlp/internal_metrics:
        protocols:
          grpc:
            endpoint: localhost:14317

    processors:

      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15

      batch:
        send_batch_max_size: 1000
        send_batch_size: 100
        timeout: 10s

      k8sattributes:
        auth_type: serviceAccount
        extract:
          #          labels:
          #            - from: pod
          #              key: app.kubernetes.io/name
          #              tag_name: kube_app_name
          #            - from: pod
          #              key: app.kubernetes.io/instance
          #              tag_name: kube_app_instance
          #            - from: pod
          #              key: app.kubernetes.io/version
          #              tag_name: kube_app_version
          #            - from: pod
          #              key: app.kubernetes.io/component
          #              tag_name: kube_app_component
          #            - from: pod
          #              key: app.kubernetes.io/part-of
          #              tag_name: kube_app_part_of
          #            - from: pod
          #              key: app.kubernetes.io/managed-by
          #              tag_name: kube_app_managed_by
          metadata:
            - container.image.name
            - container.image.tag
            - container.id
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.cronjob.name
            - k8s.container.name
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.deployment.name
            - k8s.job.name
            - k8s.node.name
            - k8s.namespace.name
            - k8s.pod.start_time
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.statefulset.name
            - k8s.statefulset.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection

      resourcedetection:
        detectors: [env, aks, k8snode ]
        override: false
        timeout: 5s
        aks:
          resource_attributes:
            k8s.cluster.name:
              enabled: true

      resource:
        attributes:
          - action: insert
            from_attribute: k8s.deployment.name
            key: service.name
          - action: insert
            from_attribute: k8s.daemonset.name
            key: service.name
          - action: insert
            from_attribute: k8s.statefulset.name
            key: service.name
          - action: insert
            from_attribute: k8s.cronjob.name
            key: service.name
          - action: insert
            from_attribute: service.name
            key: job
          - action: insert
            from_attribute: env
            key: env
          - action: upsert
            key: env
            value: ${env:STAGE}

      attributes:
        actions:
          - key: env
            action: upsert
            value: ${env:STAGE}

      attributes/logs:
        actions:
          - action: insert
            key: loki.resource.labels
            value: event.domain,k8s.namespace.name,k8s.pod.name,container.image.name,container.image.tag,k8s.deployment.name,k8s.cronjob.name,k8s.statefulset.name,k8s.job.name,k8s.daemonset.name,k8s.node.name,k8s.start.time,log.file.path
          - action: insert
            key: loki.format
            value: raw

      attributes/metrics:
        actions:
          - action: insert
            key: cluster
            from_attribute: k8s.cluster.name

      attributes/copy-client-address:
        actions:
          - action: upsert
            key: source.address
            from_attribute: client.address

      filter:
        traces:
          span:
            - 'IsMatch(attributes["url.path"], "/ping")'
            - 'IsMatch(attributes["url.path"], "/metrics")'
            - 'IsMatch(attributes["url.path"], "/api/health")'
            - 'IsMatch(attributes["url.path"], "/grafana/metrics")'
        logs:
          log_record:
            - 'IsMatch(body, "debug")'
            - 'IsMatch(body, "DEBUG")'

      geoip:
        context: record
        providers:
          maxmind:
            database_path: /geoip/GeoLite2-City.mmdb

      attributes/delete-source-address:
        actions:
          - action: delete
            key: source.address

    exporters:
      debug: {}

      otlphttp/loki:
        endpoint: http://loki-gateway.loki/otlp
        tls:
          insecure: true
        headers:
          X-Scope-OrgID: "1"

      otlphttp/mimir:
        endpoint: http://mimir-nginx.mimir/otlp
        tls:
          insecure: true

      otlp/tempo:
        endpoint: tempo.tempo:4317
        tls:
          insecure: true

    connectors:
      spanmetrics:
        histogram:
          explicit:
            buckets: [ 100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms ]
        dimensions:
          - name: http.method
            default: GET
          - name: http.status_code
          - name: span.status.code
        exemplars:
          enabled: true
        exclude_dimensions: [ 'status.code' ]
        dimensions_cache_size: 1000
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
        metrics_flush_interval: 15s
        metrics_expiration: 5m
        events:
          enabled: true
          dimensions:
            - name: exception.type
            - name: exception.message
        resource_metrics_key_attributes:
          - service.name
          - telemetry.sdk.language
          - telemetry.sdk.name

#      signaltometrics:
#        spans:
#          - name: span.count
#            description: Count of spans
#            sum:
#              value: Int(AbsoluteCount()) # Count of total spans represented by each span
#        datapoints:
#          - name: datapoint.count
#            description: Count of datapoints
#            sum:
#              value: "1" # increment by 1 for each datapoint
#        logs:
#          - name: logrecord.count
#            description: Count of log records
#            sum:
#              value: "1" # increment by 1 for each log record

      exceptions:
        dimensions:
          - name: exception.type
          - name: exception.message

#      servicegraph:
#        latency_histogram_buckets: [100ms, 250ms, 1s, 5s, 10s]
#        dimensions:
#          - dimension-1
#          - dimension-2
#        virtual_node_peer_attributes:
#          - peer.service
#          - net.peer.name
#          - db.name
#          - db.system
#        store:
#          ttl: 1s
#          max_items: 10

    extensions:
      health_check: {}

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch, memory_limiter, k8sattributes, resourcedetection, resource, attributes, attributes/copy-client-address, filter, geoip, attributes/delete-source-address ]
          exporters: [otlp/tempo, exceptions] # servicegraph, signaltometrics, spanmetrics
        metrics:
          receivers: [otlp, otlp/internal_metrics, prometheus, hostmetrics, kubeletstats, exceptions] # spanmetrics, servicegraph, signaltometrics
          processors: [batch, memory_limiter, k8sattributes, resourcedetection, attributes/metrics]
          exporters: [otlphttp/mimir]
        logs:
          receivers: [k8s_cluster, k8sobjects, filelog, k8s_events, exceptions]
          processors: [batch, memory_limiter, k8sattributes, resourcedetection, resource, attributes, attributes/logs, filter]
          exporters: [debug] #otlphttp/loki , signaltometrics

      telemetry:
        metrics:
          readers:
            - periodic:
                interval: 10000
                exporter:
                  otlp:
                    protocol: grpc/protobuf
                    endpoint: localhost:14317
